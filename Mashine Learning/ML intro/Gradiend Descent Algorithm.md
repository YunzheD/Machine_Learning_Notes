梯度下降算法更新规则

梯度下降算法通过将参数 w 更新为 w - Alpha * (d/dw J(w,b)) 来迭代调整模型参数。
等号在此上下文中表示赋值操作，与数学中的真值断言不同。
梯度下降的关键组成部分

学习率（Alpha）是一个小的正数，通常在0到1之间，它控制着每次下降的步长大小。
导数项 d/dw J(w,b) 指示了参数更新的方向，并与学习率共同决定了步长的大小。
参数的同步更新

在梯度下降中，模型的所有参数（例如 w 和 b）必须同时更新，以确保算法的正确性。
正确的实现方法是先计算所有新参数的临时值，然后将这些临时值同时赋给对应的参数，而不是顺序更新。
<img width="699" height="330" alt="image" src="https://github.com/user-attachments/assets/2368860e-d120-4b80-a9ba-c970dac59a55" />

导数的作用
导数表示函数在某一点的斜率，决定了参数w的更新方向。
正斜率时，更新w会减小；负斜率时，更新w会增大，从而逐步接近成本函数的最小值。
<img width="693" height="360" alt="image" src="https://github.com/user-attachments/assets/4ff35f82-5a38-42e4-8211-b9fc1a98d25d" />
学习率的影响

学习率过小：如果学习率设置得过小，梯度下降会以非常小的步伐更新参数，导致收敛速度极慢，需要很多步骤才能接近最小值。
学习率过大：如果学习率设置得过大，可能会导致参数更新过度，甚至使得成本函数的值变得更高，无法收敛，甚至发散。
局部最小值的情况

当参数已经达到局部最小值时，梯度下降的更新不会改变参数值，因为此时导数为零，保持在当前值不变。
